#!/usr/bin/env python3
"""
Chisel Diff Corrector - Ensures LLM-generated diffs use exact removal lines from hints

Purpose:
    LLMs often recreate code from understanding instead of copying from hints,
    causing formatting/indentation mismatches. This corrector ensures removal
    lines in generated diffs match the exact lines from hints.

Usage:
    corrector = ChiselDiffCorrector(confidence_threshold=0.90)
    result = corrector.correct_diff(
        generated_diff=llm_output,
        hints=chisel_hints,
        verilog_diff=original_verilog_diff
    )

    if result['is_ambiguous']:
        # Need LLM clarification - multiple matches found
        ambiguous_info = result['ambiguous_lines']
"""

import re
from typing import Dict, List, Tuple, Set


class ChiselDiffCorrector:
    """Corrects LLM-generated Chisel diffs to use exact removal lines from hints"""

    def __init__(self, confidence_threshold: float = 0.90):
        """
        Initialize corrector

        Args:
            confidence_threshold: Minimum confidence (0.0-1.0) to auto-correct
                                 0.85 = aggressive, 0.90 = moderate, 0.95 = conservative
        """
        self.confidence_threshold = confidence_threshold
        self.correction_log = []

    def correct_diff(self, generated_diff: str, hints: str, verilog_diff: str = None) -> Dict:
        """
        Main entry point - correct diff removal lines to match hints exactly

        Args:
            generated_diff: The unified diff generated by LLM
            hints: The Chisel code hints provided to LLM
            verilog_diff: Original Verilog diff (for context/debugging)

        Returns:
            {
                'success': bool,
                'corrected_diff': str,
                'corrections_made': int,
                'ambiguous_lines': [...],
                'no_match_lines': [...],
                'is_ambiguous': bool,
                'correction_log': [...]
            }
        """
        self.correction_log = []
        hints_lines = hints.splitlines()

        # Step 1: Parse diff and extract removal lines with context
        removal_blocks = self._parse_removal_lines(generated_diff)

        if not removal_blocks:
            # No removal lines - nothing to correct
            return {
                'success': True,
                'corrected_diff': generated_diff,
                'corrections_made': 0,
                'ambiguous_lines': [],
                'no_match_lines': [],
                'is_ambiguous': False,
                'correction_log': [],
            }

        # Step 2: Find matches in hints for each removal line
        corrections = {}
        ambiguous_lines = []
        no_match_lines = []
        corrections_made = 0

        for block in removal_blocks:
            for removal_line in block['removal_lines']:
                line_text = removal_line['text']
                hunk_index = removal_line['hunk_index']
                line_index = removal_line['line_index']

                # Skip empty lines or diff markers
                if not line_text.strip() or line_text.strip().startswith('---') or line_text.strip().startswith('+++'):
                    continue

                # Find matching lines in hints
                match_result = self._find_matching_hint_lines(line_text, hints_lines)

                if match_result['is_unique']:
                    # Unique high-confidence match - auto-correct
                    best_match = match_result['best_match']
                    corrections[(hunk_index, line_index)] = best_match
                    corrections_made += 1

                    self.correction_log.append(
                        {
                            'original': line_text,
                            'corrected': best_match,
                            'confidence': match_result['matches'][0]['confidence'],
                            'method': 'fuzzy_match',
                        }
                    )

                elif match_result['is_ambiguous']:
                    # Multiple matches - need clarification
                    ambiguous_lines.append(
                        {
                            'original': line_text,
                            'matches': match_result['matches'],
                        }
                    )

                    self.correction_log.append(
                        {'original': line_text, 'corrected': None, 'confidence': 0.0, 'method': 'ambiguous'}
                    )

                else:
                    # No match found - LLM error or line not in hints
                    no_match_lines.append(line_text)

                    self.correction_log.append(
                        {'original': line_text, 'corrected': None, 'confidence': 0.0, 'method': 'no_match'}
                    )

        # Step 3: Rebuild diff with corrections
        corrected_diff = self._rebuild_diff_with_corrections(generated_diff, corrections)

        return {
            'success': len(ambiguous_lines) == 0 and len(no_match_lines) == 0,
            'corrected_diff': corrected_diff,
            'corrections_made': corrections_made,
            'ambiguous_lines': ambiguous_lines,
            'no_match_lines': no_match_lines,
            'is_ambiguous': len(ambiguous_lines) > 0,
            'correction_log': self.correction_log,
        }

    def _parse_removal_lines(self, diff: str) -> List[Dict]:
        """
        Parse unified diff and extract all removal lines with context

        Returns list of hunks with removal lines:
        [
            {
                'hunk_header': str,
                'hunk_index': int,
                'removal_lines': [
                    {'text': str, 'line_index': int, 'hunk_index': int}
                ]
            }
        ]
        """
        diff_lines = diff.splitlines()
        hunks = []
        current_hunk = None
        hunk_index = -1
        line_index = 0

        for i, line in enumerate(diff_lines):
            # Detect hunk header (@@)
            if line.lstrip().startswith('@@'):
                if current_hunk is not None:
                    hunks.append(current_hunk)

                hunk_index += 1
                current_hunk = {
                    'hunk_header': line,
                    'hunk_index': hunk_index,
                    'removal_lines': [],
                }
                line_index = 0

            elif current_hunk is not None:
                # Inside a hunk
                if line.startswith('-') and not line.startswith('---'):
                    # Removal line
                    removal_text = line[1:]  # Remove the '-' prefix
                    current_hunk['removal_lines'].append(
                        {
                            'text': removal_text,
                            'line_index': line_index,
                            'hunk_index': hunk_index,
                        }
                    )
                line_index += 1

        # Add last hunk
        if current_hunk is not None:
            hunks.append(current_hunk)

        return hunks

    def _find_matching_hint_lines(self, removal_line: str, hints_lines: List[str]) -> Dict:
        """
        Find matching line(s) in hints using fuzzy matching

        Returns:
        {
            'matches': [{'line': str, 'line_num': int, 'confidence': float}],
            'best_match': str or None,
            'is_unique': bool,      # True if exactly 1 high-confidence match
            'is_ambiguous': bool,   # True if multiple high-confidence matches
        }
        """
        matches = []

        # Normalize removal line for comparison
        normalized_removal = self._normalize_for_matching(removal_line)

        # Search all hint lines
        for line_num, hint_line in enumerate(hints_lines):
            normalized_hint = self._normalize_for_matching(hint_line)

            # Calculate similarity
            confidence = self._calculate_similarity(normalized_removal, normalized_hint)

            if confidence >= self.confidence_threshold:
                matches.append({'line': hint_line, 'line_num': line_num + 1, 'confidence': confidence})

        # Sort by confidence (highest first)
        matches.sort(key=lambda x: x['confidence'], reverse=True)

        # Determine match status
        is_unique = len(matches) == 1
        is_ambiguous = len(matches) > 1
        best_match = matches[0]['line'] if matches else None

        return {
            'matches': matches,
            'best_match': best_match,
            'is_unique': is_unique,
            'is_ambiguous': is_ambiguous,
        }

    def _normalize_for_matching(self, line: str) -> str:
        """
        Normalize line for fuzzy matching

        Normalizations:
        - Remove comments
        - Normalize whitespace
        - Normalize operators
        - Strip leading/trailing space
        """
        # Remove trailing comments
        if '//' in line:
            line = line.split('//')[0]

        # Normalize operators (handle multi-char first)
        line = re.sub(r'\s*===\s*', ' === ', line)
        line = re.sub(r'\s*:=\s*', ' := ', line)
        line = re.sub(r'(?<!:)(?<!=)\s*=\s*(?!=)', ' = ', line)

        # Normalize punctuation
        line = re.sub(r'\s*,\s*', ', ', line)
        line = re.sub(r'\s*\(\s*', '(', line)
        line = re.sub(r'\s*\)\s*', ') ', line)
        line = re.sub(r'\s*\[\s*', '[', line)
        line = re.sub(r'\s*\]\s*', '] ', line)
        line = re.sub(r'\s*\{\s*', '{ ', line)
        line = re.sub(r'\s*\}\s*', '} ', line)
        line = re.sub(r'\s*\.\s*', '.', line)

        # Collapse multiple spaces
        line = re.sub(r'\s+', ' ', line)

        return line.strip()

    def _extract_signature(self, line: str) -> Set[str]:
        """
        Extract meaningful tokens for similarity calculation

        Tokens:
        - Keywords: when, if, val, def, else, otherwise
        - Identifiers: io.funct7, operation, result
        - Literals: "b000000", 0.U, 5, true, false
        - Operators: ===, :=, &&, ||
        """
        # Tokenize by spaces and special chars
        tokens = re.findall(r'\w+|===|:=|&&|\|\||[!=<>]=?|[+\-*/]', line)

        # Filter meaningful tokens (skip very short generic ones)
        meaningful = set()
        for token in tokens:
            # Keep operators
            if re.match(r'===|:=|&&|\|\||[!=<>]=?', token):
                meaningful.add(token)
            # Keep keywords
            elif token in ['when', 'if', 'val', 'def', 'else', 'otherwise', 'elsewhen', 'true', 'false']:
                meaningful.add(token)
            # Keep identifiers and literals (length > 1)
            elif len(token) > 1:
                meaningful.add(token)

        return meaningful

    def _calculate_similarity(self, line1: str, line2: str) -> float:
        """
        Calculate similarity score between two normalized lines

        Score = (matching_tokens) / (total_unique_tokens)
        Range: 0.0 (no match) to 1.0 (identical)
        """
        sig1 = self._extract_signature(line1)
        sig2 = self._extract_signature(line2)

        if not sig1 and not sig2:
            # Both empty
            return 1.0 if line1.strip() == line2.strip() else 0.0

        if not sig1 or not sig2:
            # One empty, one not
            return 0.0

        # Calculate Jaccard similarity
        intersection = len(sig1 & sig2)
        union = len(sig1 | sig2)

        if union == 0:
            return 0.0

        return intersection / union

    def _rebuild_diff_with_corrections(self, original_diff: str, corrections: Dict[Tuple[int, int], str]) -> str:
        """
        Rebuild diff with corrected removal lines

        Args:
            original_diff: Original LLM-generated diff
            corrections: Map of (hunk_index, line_index) -> corrected_line

        Returns:
            Corrected diff string
        """
        diff_lines = original_diff.splitlines()
        result_lines = []
        hunk_index = -1
        line_index = 0

        for i, line in enumerate(diff_lines):
            # Detect hunk header
            if line.lstrip().startswith('@@'):
                hunk_index += 1
                line_index = 0
                result_lines.append(line)

            elif line.startswith('-') and not line.startswith('---'):
                # Removal line - check if we have a correction
                key = (hunk_index, line_index)
                if key in corrections:
                    # Use corrected line (add '-' prefix back)
                    corrected_line = corrections[key]
                    result_lines.append('-' + corrected_line)
                else:
                    # Keep original
                    result_lines.append(line)
                line_index += 1

            else:
                # Keep all other lines as-is
                result_lines.append(line)
                if hunk_index >= 0:
                    line_index += 1

        return '\n'.join(result_lines)
